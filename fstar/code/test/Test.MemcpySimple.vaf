include "../arch/x64simple/X64s.Vale.InsBasic.vaf"
include "../arch/x64simple/X64s.Vale.InsMem.vaf"
include "../arch/x64simple/X64s.Vale.InsVector.vaf"

module Test.MemcpySimple

#verbatim{:interface}{:implementation}
open Defs_s
open X64.Machine_s
open X64s.Vale.State
open X64s.Vale.Decls
open X64s.Vale.InsBasic
open X64s.Vale.InsMem
open X64s.Vale.InsVector
#endverbatim

procedure Copy16()
    {:options z3rlimit(50)}
    requires/ensures
        valid_mem64(rsi, mem);
        valid_mem64(rsi + 8, mem);
        valid_mem64(rdi, mem);
        valid_mem64(rdi + 8, mem);
        rsi + 16 <= rdi || rdi + 16 <= rsi;
    ensures
        forall(i) 0 <= i && i < 16 && i % 8 = 0 ==> load_mem64(rsi + i, mem) == load_mem64(rdi + i, mem);
    reads
        rsi; rdi;
    modifies
        rax; rbx;
        mem;
{
    Mov64(rax, Mem64(rsi, 0)); // equivalent to: Load64(rax, rsi, 0);
    Mov64(rbx, Mem64(rsi, 8)); // equivalent to: Load64(rbx, rsi, 8);
    Mov64(Mem64(rdi, 0), rax); // equivalent to: Store64(rdi, rax, 0);
    Mov64(Mem64(rdi, 8), rbx); // equivalent to: Store64(rdi, rbx, 8);
}

procedure Copy16Q()
    {:options z3rlimit(50)}
    requires/ensures
        valid_mem64(rsi, mem);
        valid_mem64(rsi + 8, mem);
        valid_mem64(rdi, mem);
        valid_mem64(rdi + 8, mem);
        rsi + 16 <= rdi || rdi + 16 <= rsi;
    ensures
        forall(i) 0 <= i && i < 16 && i % 8 = 0 ==> load_mem64(rsi + i, mem) == load_mem64(rdi + i, mem);
    reads
        rsi; rdi;
    modifies
        rax; rbx;
        mem;
{
    Mov64(rax, Mem64(rsi, 0)); // equivalent to: Load64(rax, rsi, 0);
    Mov64(rbx, Mem64(rsi, 8)); // equivalent to: Load64(rbx, rsi, 8);
    Mov64(Mem64(rdi, 0), rax); // equivalent to: Store64(rdi, rax, 0);
    Mov64(Mem64(rdi, 8), rbx); // equivalent to: Store64(rdi, rbx, 8);
}

procedure benchmark_quick_modifies(
        inline win:bool,
        ghost old_xmm6:quad32,
        ghost old_xmm7:quad32,
        ghost old_xmm8:quad32,
        ghost old_xmm9:quad32,
        ghost old_xmm10:quad32,
        ghost old_xmm11:quad32,
        ghost old_xmm12:quad32,
        ghost old_xmm13:quad32,
        ghost old_xmm14:quad32,
        ghost old_xmm15:quad32
        )
    {:options z3rlimit(50)}
    requires
        win ==> old_xmm15 == xmm6;
        win ==> old_xmm15 == xmm6;
        win ==> old_xmm14 == xmm6;
        win ==> old_xmm14 == xmm6;
        win ==> old_xmm13 == xmm6;
        win ==> old_xmm13 == xmm6;
        win ==> old_xmm12 == xmm6;
        win ==> old_xmm12 == xmm6;
        win ==> old_xmm11 == xmm6;
        win ==> old_xmm11 == xmm6;
        win ==> old_xmm10 == xmm6;
        win ==> old_xmm10 == xmm6;
        win ==> old_xmm9  == xmm6;
        win ==> old_xmm9  == xmm6;
        win ==> old_xmm8  == xmm6;
        win ==> old_xmm8  == xmm6;
        win ==> old_xmm7  == xmm6;
        win ==> old_xmm7  == xmm6;
        win ==> old_xmm6  == xmm6;
        win ==> old_xmm6  == xmm6;
    modifies
        efl;
        rax; rbx; rbp; rdi; rsi; r12; r13; r14; r15;
        xmm6; xmm7; xmm8; xmm9; xmm10; xmm11; xmm12; xmm13; xmm14; xmm15;
        rsp;
    ensures
        rbx == old(rbx);
        rbp == old(rbp);
        rdi == old(rdi);
        rsi == old(rsi);
        r12 == old(r12);
        r13 == old(r13);
        r14 == old(r14);
        r15 == old(r15);

        win ==> xmm6  == old_xmm6  || xmm6  != old_xmm6 ;
        win ==> xmm7  == old_xmm7  || xmm7  != old_xmm7 ;
        win ==> xmm8  == old_xmm8  || xmm8  != old_xmm8 ;
        win ==> xmm9  == old_xmm9  || xmm9  != old_xmm9 ;
        win ==> xmm10 == old_xmm10 || xmm10 != old_xmm10;
        win ==> xmm11 == old_xmm11 || xmm11 != old_xmm11;
        win ==> xmm12 == old_xmm12 || xmm12 != old_xmm12;
        win ==> xmm13 == old_xmm13 || xmm13 != old_xmm13;
        win ==> xmm14 == old_xmm14 || xmm14 != old_xmm14;
        win ==> xmm15 == old_xmm15 || xmm15 != old_xmm15;
{
    Add64(rbx, 0);
    Add64(rbp, 0);
    Add64(rdi, 0);
    Add64(rsi, 0);
    Add64(r12, 0);
    Add64(r13, 0);
    Add64(r14, 0);
    Add64(r15, 0);

    inline if (win) {
        Pxor(xmm6,  xmm6);
        Pxor(xmm7,  xmm6);
        Pxor(xmm8,  xmm6);
        Pxor(xmm9,  xmm6);
        Pxor(xmm10, xmm6);
        Pxor(xmm11, xmm6);
        Pxor(xmm12, xmm6);
        Pxor(xmm13, xmm6);
        Pxor(xmm14, xmm6);
        Pxor(xmm15, xmm6);
    }
}

